# 集群cluster
    当后端服务器承受不住访问的压力，可以通过升级单台服务器能力支撑大访问压力，但是这种解决方案会极大增加成本。另一种方式是通过横向扩展增加一台或几台服务器，提供相同的服务，通过前段分发器将访问量均匀的分配到后台服务器上。这种多台服务器组成的数组集合就叫做集群。
    集群按功能划分有三种模型：
>     **负载均衡集群（loadBalance）**:  提高应用系统的响应能力、尽可能处理更多的访问请求、减少延迟为目标，获得高并发、高负载的整体性能。LB 的负载分配依赖于主节点的分流算法，将来自客户机的访问请求分担给多个服务器节点，从而缓解整个系统的负载压力。例如：DNS 轮询、反向代理等。
>     **高可用性集群（High Availability）**： 提高应用系统的可靠性、尽可能地减少中断时间为目标，确保服务的连续性，达到高可用（HA）的容错效果。HA 的工作方式包括双工和主从两种模式，双工即所有节点同时在线，主从则只有主节点在线，但当出现故障时从节点能自动切换为主节点。例如：故障切换、双机热备等。
>     **高性能集群（High Performance）:** 以提高应用系统的 CPU 运算速度、扩展硬件资源和分析能力为目标，获得相当于大型、超级计算机的高性能运算（HPC）能力。高性能依赖于 “分布式运算”、“并行计算”，通过专用硬件和软件将多个服务器的 CPU、内存等资源整合在一起，实现只有大型、超级计算机才具备的计算能力。例如：云计算、网格计算等。

#  **负载均衡集群（loadBalance）**
## 服务端负载均衡
　服务端负载均衡,根据软硬件又可分为硬件负载均衡和软件负载均衡.根据网络模型分层分类,又可以分为四层负载均衡和七层负载均衡。
![image.png](https://cdn.nlark.com/yuque/0/2022/png/992017/1659254832963-047a8755-506b-48da-83cf-a5f92b48ec66.png#clientId=u064f696c-3abb-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=259&id=ua5f37978&margin=%5Bobject%20Object%5D&name=image.png&originHeight=518&originWidth=1024&originalType=url&ratio=1&rotation=0&showTitle=false&size=102366&status=done&style=none&taskId=u6a89f501-2a4d-43b1-bf85-f4d3bc600cb&title=&width=512)
### 分类
####  根据软硬件分类
> **硬件负载均衡**：通过硬件负载均衡设备,实现服务器间的负载均衡.常见的如:F5等设备.特点是高性能,高成本。
**软件负载均衡**：通过部署负载均衡软件,实现服务器间的负载均衡.常见的如:LVS,NGINX,HAPROXY.特点是低成本,但是性能较差。

#### 根据网络模型分层分类
根据网络协议的OSI七层网络模型，负载均衡的网络消息转发一般集中在传输层和应用层。对于OSI网络分层的第四层和第七层，所有又称作四层负载均衡和七层负载均衡技术。
**四层负载均衡：**主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。以常见的TCP为例:负载均衡设备在接收到第一个来自客户端的SYN 请求->负载均衡算法选择服务器->修改报文目标IP地址->转发给实际的服务器。
**七层负载均衡：** 反向代理.以常见的TCP为例:负载均衡设备通过解析报文中应用层的内容,将请求转发给合适的服务器.负载均衡设备先通过三次握手分别与所代理的应用服务器和客户端建立联系->解析客户端发送的应用层报文->负载均衡算法->决定最终选择的内部服务器。
### 负载均衡集群结构
![image.png](https://cdn.nlark.com/yuque/0/2022/png/992017/1659255115274-fa1591db-a8b7-4751-8f49-176509095815.png#clientId=u064f696c-3abb-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=287&id=u7da9020a&margin=%5Bobject%20Object%5D&name=image.png&originHeight=573&originWidth=774&originalType=url&ratio=1&rotation=0&showTitle=false&size=65145&status=done&style=none&taskId=ufa02885a-9f47-44f2-b79d-0b198d7d6ee&title=&width=387)
**负载调度器（Load Balancer 或 Director）：**访问整个群集系统的唯一入口，对外使用所有服务器公有的 VIP 地址，也称为群集 IP 地址。通常会配置主、备两台调度器实现热备份，当主调度器失效以后能够平滑替换至备用调度器，确保高可用性。
**服务器池（Server Pool）：**群集所提供的应用服务，由服务器池承担，其中每个节点具有独立的 RIP 地址（真实 IP），只处理调度器分发过来的客户机请求。当某个节点暂时失效时，堵在调度器的容错机制会将其隔离，等待错误排除以后再重新纳入服务器池。
**共享存储（Share Storage）：**为服务器池中的所有节点提供稳定、一致的文件存取服务，确保整个群集的统一性。共享存储可以使用 NAS 设备，或者提供 NFS 共享服务的专用服务器。
### 工作模式
[**地址转换(NAT模式)**](http://www.linux-vs.org/VS-NAT.html)**：**Network Address Translation，简称NAT模式。类似于防火墙的私有网络结构，负责调度器作为所有服务器节点的网关，及作为客户机的访问入口也是个节点回应客户机的访问出口，也就是说这个负载调度器会承受双倍的负载压力，服务器节点使用私有IP地址转换，与负载调度器位于同一个物理网络，安全性要优于其他两种模式。
**IP隧道（TUN模式）：**IP 隧道（IP Tunnel，简称 TUN 模式），采用开放式的网络结构，负载调度器仅作为客户机的访问入口，各节点通过各自的 Internet 连接直接回应客户机，而不再经过负载调度器。服务器节点分散在互联网中的不同位置，具有独立的公网 IP 地址，通过专用 IP 隧道与负载调度器通信。该模式需要大量的公网 IP，因此使用较少，常用于大型企业的异地灾备。
**直接路由（DR模式）：**采用半开放式的网络结构，与 TUN 模式的结构类似，但各节点并不是分散在各地，而是与调度器位于同一个物理网络。负载调度器与各节点服务器通过本地网络连接，不需要建立专用的 IP 隧道。兼具安全性和访问性，是使用最多的一种模式。
### 负载均衡算法
Different load balancing algorithms provide different benefits; the choice of load balancing method depends on your needs:

- **Round Robin** – Requests are distributed across the group of servers sequentially.
- **Least Connections** – A new request is sent to the server with the fewest current connections to clients. The relative computing capacity of each server is factored into determining which one has the least connections.
- **Least Time** – Sends requests to the server selected by a formula that combines the
fastest response time and fewest active connections. Exclusive to NGINX Plus.
- **Hash** – Distributes requests based on a key you define, such as the client IP address or
the request URL. NGINX Plus can optionally apply a consistent hash to minimize redistribution
of loads if the set of upstream servers changes.
- **IP Hash** – The IP address of the client is used to determine which server receives the request.
- **Random with Two Choices** – Picks two servers at random and sends the request to the
one that is selected by then applying the Least Connections algorithm (or for NGINX Plus
the Least Time algorithm, if so configured).
### LVS
LVS（Linux Virtual Server）即Linux虚拟服务器，是由章文嵩博士主导的开源负载均衡项目，已经被集成到Linux内核模块中。该项目在Linux内核中实现了基于IP的数据请求负载均衡调度方案，其体系结构如图所示，
![image.png](https://cdn.nlark.com/yuque/0/2022/png/992017/1659256489237-ca9bdde5-57f7-4a37-b572-48a14fd01c11.png#clientId=u064f696c-3abb-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=294&id=u72f9a166&margin=%5Bobject%20Object%5D&name=image.png&originHeight=588&originWidth=652&originalType=url&ratio=1&rotation=0&showTitle=false&size=20909&status=done&style=none&taskId=ua50b0f91-b786-4593-9fa4-a6e9bee3f78&title=&width=326)
互联网用户从外部访问公司的外部负载均衡服务器，用户的Web请求会发送给LVS调度器，调度器根据预设的算法决定将该请求发送给后端的某台Web服务器。根据LVS工作模式的不同，真实服务器会选择不同的方式将用户需要的数据发送到用户。LVS工作模式分为NAT模式、TUN模式、以及DR模式：
> - [Virtual Server via NAT](http://www.linux-vs.org/VS-NAT.html)
> - [Virtual Server via IP Tunneling](http://www.linux-vs.org/VS-IPTunneling.html)
> - [Virtual Server via Direct Routing](http://www.linux-vs.org/VS-DRouting.html)

LVS三种工作模式对比如下：

|  | VS/NAT | VS/TUN | VS/DR |
| --- | --- | --- | --- |
| server | any | tunneling | non-arp device |
| server network | private | LAN/WAN | LAN |
| server number | low (10~20) | high | high |
| server gateway | load balancer | own router | own router |

## 客户端负载均衡
  //todo
# 参考文章 



、

